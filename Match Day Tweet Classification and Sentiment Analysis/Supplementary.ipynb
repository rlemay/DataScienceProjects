{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtCa0sZ8EXdj"
   },
   "source": [
    "**Set Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyMMD_upEfIa",
    "outputId": "e41ba723-d593-4f7d-e41e-77158405bd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_292\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~18.04-b10)\n",
      "OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)\n",
      "Collecting pyspark==2.4.7\n",
      "  Downloading pyspark-2.4.7.tar.gz (217.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.9 MB 45 kB/s \n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 19.1 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-2.4.7-py2.py3-none-any.whl size=218279466 sha256=dc4bff992861a6a116e6749ed7a5df39463dc30f80c4368c96e4d114090bd383\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/28/74/56054e5fe3413c8c58b67e4d7483d4864fe483920c9b8ec754\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.7\n",
      "Collecting spark-nlp==2.7.3\n",
      "  Downloading spark_nlp-2.7.3-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 5.2 MB/s \n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-2.7.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Pyspark\n",
    "! pip install --ignore-installed pyspark==2.4.7\n",
    "\n",
    "# Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==2.7.3\n",
    "\n",
    "# SparkSession start\n",
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7eWkkV7Ohd-u"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "## Pretrained Pipeline from Spark NLP\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.base import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # Start Spark Session with Spark NLP\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HdKqjQRChzsI"
   },
   "outputs": [],
   "source": [
    "# To use Spark and its API import the SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "# Create Spark Context\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmUgC_G1UcCG"
   },
   "source": [
    "**Compile Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssK1h2-sVNc8",
    "outputId": "2284628c-39cd-4a3e-d263-717f5699fa7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11382"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df_full = sqlContext.read.format('csv').option(\"delimiter\",\",\").option(\"header\",\"true\").load('/content/samp3_3.csv')\n",
    "\n",
    "# verify 11,382 records\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M6VZpEKiVa0Z",
    "outputId": "9f5f2fd3-dd3c-4a62-9a06-c3f391baed15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n",
      "11382\n"
     ]
    }
   ],
   "source": [
    "# select needed columns\n",
    "df = df_full.select(col(\"label\"),col(\"clnNoMent\"))\n",
    "# drop na\n",
    "df_label = df.na.drop()\n",
    "print(df_label.count())\n",
    "# for all\n",
    "df_all = df_full.select(col(\"match2022\"),col(\"clnNoMent\"))\n",
    "print(df_all.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqx6i4Eoysh-"
   },
   "source": [
    "**Pipeline Set Up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "iN_mFtfOhmNI"
   },
   "outputs": [],
   "source": [
    "# taken from homework with small modifications\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, SQLTransformer,IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\") # convert text column to spark nlp document\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\") # convert document to array of tokens\n",
    " \n",
    "# clean tokens \n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols(\"normalized\").setOutputCol(\"cleanTokens\").setCaseSensitive(False) # remove stopwords\n",
    "stemmer = Stemmer().setInputCols([\"cleanTokens\"]).setOutputCol(\"stem\") # stem\n",
    "finisher = Finisher().setInputCols([\"stem\"]).setOutputCols([\"token_features\"]).setOutputAsArray(True).setCleanAnnotations(False)\n",
    "finisher2 = Finisher().setInputCols([\"token\"]).setOutputCols([\"token_features\"]).setOutputAsArray(True).setCleanAnnotations(False)# TF\n",
    "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\") # TF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") #IDF\n",
    "label_stringIdx = StringIndexer(inputCol = \"category\", outputCol = \"label\") # convert labels (string) to integers\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.2, elasticNetParam=0.0) # LR model\n",
    "svm = LinearSVC(maxIter=10, regParam=0.2) #SVM model \n",
    "svm2 = LinearSVC(regParam=0.455) #SVM2 model \n",
    "label_to_stringIdx = IndexToString(inputCol=\"label\", outputCol=\"article_class\") # convert index(integer) to corresponding class labels\n",
    "# lr pipeline\n",
    "lr_pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, finisher, hashingTF, idf, label_stringIdx, lr, label_to_stringIdx])\n",
    "# svm pipeline\n",
    "# no additional text cleaning\n",
    "svm3_pipeline = Pipeline(stages=[document_assembler, tokenizer, finisher2, hashingTF, idf, label_stringIdx, svm2, label_to_stringIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "E5Q6pfmfxTX8"
   },
   "outputs": [],
   "source": [
    "# evaluation + validation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, GridSearchCV, ShuffleSplit\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "0tGlb9bdx19F"
   },
   "outputs": [],
   "source": [
    "# function to display 2 decimals of float\n",
    "def two_d(num):\n",
    "  return (\"{:.4f}\".format(num))\n",
    "\n",
    "# second function to print precision, recall, f1, and accuracy\n",
    "def eval_metrics(p_labels,t_labels):\n",
    "  pre = precision_score(p_labels,t_labels)\n",
    "  rec = recall_score(p_labels,t_labels)\n",
    "  f1 = f1_score(p_labels,t_labels)\n",
    "  acc = accuracy_score(p_labels,t_labels)\n",
    "  return pre, rec, f1, acc\n",
    "\n",
    "# for repeated printing\n",
    "def eval_metrics_print(pr, re, fone, ac):\n",
    "  print('Precision: '+two_d(pr))\n",
    "  print('Recall: '+two_d(re))\n",
    "  print('F1: '+two_d(fone))\n",
    "  print('Accuracy: '+two_d(ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Jvj93TbfmzSn"
   },
   "outputs": [],
   "source": [
    "# 70/30 spit for training and test data\n",
    "(trainingData, testData) = df_label.withColumnRenamed('clnNoMent', 'text').withColumnRenamed('label', 'category').randomSplit([0.7, 0.3], seed = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNNQ1n0TqQW1"
   },
   "source": [
    "**LR Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "v7ZYz5WglUli"
   },
   "outputs": [],
   "source": [
    "lr_pipeline_model = lr_pipeline.fit(trainingData)\n",
    "lrpredictions =  lr_pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emDyN5cko_0t"
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "BkAgC549pAx2"
   },
   "outputs": [],
   "source": [
    "# create pandasDF\n",
    "pandasDF = lrpredictions.select(['prediction', 'label']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "EjJVf4snpLxo"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# create function to get probs\n",
    "secondelement = udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "# add probs to df\n",
    "lrprobs = lrpredictions.select(secondelement('probability')).toPandas()\n",
    "lrresults = pandasDF.join(lrprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWZj7-NypnXz",
    "outputId": "374005bb-3f82-4e52-d7b6-091eb68b6566"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6667\n",
      "Recall: 0.2857\n",
      "F1: 0.4000\n",
      "Accuracy: 0.9259\n",
      "AUC ROC: 0.8359\n"
     ]
    }
   ],
   "source": [
    "#print metrics\n",
    "lrpre, lrrec, lrf1, lracc = eval_metrics(lrresults['label'], lrresults['prediction'])\n",
    "eval_metrics_print(lrpre, lrrec, lrf1, lracc)\n",
    "\n",
    "# AUC ROC\n",
    "lr_roc_auc_scor = roc_auc_score(lrresults['label'], lrprobs)\n",
    "print('AUC ROC: '+two_d(lr_roc_auc_scor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHWNzAcjugmA"
   },
   "source": [
    "Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "r81TVnlgun5f"
   },
   "outputs": [],
   "source": [
    "df_all_F = df_all.withColumnRenamed('clnNoMent', 'text').withColumnRenamed('match2022', 'category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "gNbFM98VuiCS"
   },
   "outputs": [],
   "source": [
    "lrFullpredictions =  lr_pipeline_model.transform(df_all_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "XreVwi2Rvhfk"
   },
   "outputs": [],
   "source": [
    "# create pandas DF\n",
    "pandasDF2 = lrFullpredictions.select(['prediction', 'label']).toPandas()\n",
    "\n",
    "# add probs to df\n",
    "lrFprobs = lrFullpredictions.select(secondelement('probability')).toPandas()\n",
    "lrFresults = pandasDF.join(lrFprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kdyqd-WrvL5i",
    "outputId": "c8f42264-0980-44e8-f41f-7c95c09c3d36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4792\n",
      "Recall: 0.2081\n",
      "F1: 0.2902\n",
      "Accuracy: 0.9802\n",
      "AUC ROC: 0.9588\n"
     ]
    }
   ],
   "source": [
    "#print metrics\n",
    "lrpr, lrre, lrf, lrac = eval_metrics(pandasDF2['label'], pandasDF2['prediction'])\n",
    "eval_metrics_print(lrpr, lrre, lrf, lrac)\n",
    "\n",
    "# AUC ROC\n",
    "lr_roc_auc_sco = roc_auc_score(pandasDF2['label'], lrFprobs)\n",
    "print('AUC ROC: '+two_d(lr_roc_auc_sco))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxk4rX2wqvKI"
   },
   "source": [
    "**SVM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "Q8QGfG1HqvKJ"
   },
   "outputs": [],
   "source": [
    "svm_pipeline_model = svm3_pipeline.fit(trainingData)\n",
    "svmpredictions =  svm_pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBwJ6G9EqvKJ"
   },
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "-d7Nlw4uqvKJ"
   },
   "outputs": [],
   "source": [
    "# create pandasDR\n",
    "svmresults = svmpredictions.select(['prediction', 'label']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7c8BkmLqvKJ",
    "outputId": "0ee30538-cf81-462f-f668-13cc88130fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8571\n",
      "Recall: 0.4286\n",
      "F1: 0.5714\n",
      "Accuracy: 0.9444\n"
     ]
    }
   ],
   "source": [
    "#print metrics\n",
    "svmpre, svmrec, svmf1, svmacc = eval_metrics(svmresults['label'], svmresults['prediction'])\n",
    "eval_metrics_print(svmpre, svmrec, svmf1, svmacc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8eDJkCUx0vs"
   },
   "source": [
    "Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "9REuWBDox0vt"
   },
   "outputs": [],
   "source": [
    "svmFullpredictions =  svm_pipeline_model.transform(df_all_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "yCh2FPtPx0vu"
   },
   "outputs": [],
   "source": [
    "# create pandasDR\n",
    "svmFullresults = svmFullpredictions.select(['prediction', 'label']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEWVelt_x0vu",
    "outputId": "3d982fda-07ed-4991-a2fa-d778bfe93c39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4329\n",
      "Recall: 0.5837\n",
      "F1: 0.4971\n",
      "Accuracy: 0.9771\n"
     ]
    }
   ],
   "source": [
    "#print metrics\n",
    "lrpr, lrre, lrf, lrac = eval_metrics(svmFullresults['label'], svmFullresults['prediction'])\n",
    "eval_metrics_print(lrpr, lrre, lrf, lrac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "IsGzsSO8y7Sa"
   },
   "outputs": [],
   "source": [
    "svmFullresults.to_csv('PySpSVMResults.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProjectSupplementary_LeMay.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
